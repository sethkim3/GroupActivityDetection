from sklearn import linear_model
import pandas as pd
import numpy as np
import networkx as nx
from itertools import combinations

def engineer_features(samples_df, n_samples, n_individuals, features):
    """
    Engineer features from the samples dataframe and return a new dataframe with the number of rows equal to the number
    of samples.
    :param samples_df: Dataframe of samples generated by generate_samples function.
    :param n_samples: The number of samples in the dataframe.
    :param n_individuals: The number of individuals per sample.
    :param features: A list of features to engineer.
    :return: A dataframe with engineered features appended, number of rows = number of samples, number of cols =
    number of features + 2 (sample_id and label).
    """

    columns = ['sample_id'] + features + ['label']

    features_df = pd.DataFrame(columns=columns)

    # create a distance network (if necessary)
    network = False
    for feature in features:
        if ('graph' in feature):
            network = True
    if (network):
        G = nx.Graph()
        edges_with_weights = []
        individual_combos = combinations(list(range(n_individuals)), 2)
        for i, j in individual_combos:
            x_dist = samples_df.iloc[i]['x_pos'] - samples_df.iloc[j]['x_pos']
            y_dist = samples_df.iloc[i]['y_pos'] - samples_df.iloc[j]['y_pos']
            tot_distance = pow(pow(x_dist, 2) + pow(y_dist, 2), 0.5)
            edges_with_weights.append((i, j, tot_distance))
        G.add_weighted_edges_from(edges_with_weights)

    for sample_id in range(n_samples):
        start_iloc = n_individuals * sample_id
        end_iloc = n_individuals * (sample_id + 1)

        row = [samples_df.iloc[start_iloc]['group_num']]

        # pre-compute some things to speed up feature engineering
        individual_combos = combinations(list(range(n_individuals)), 2)

        distances = []
        edges_with_weights = []
        for i, j in individual_combos:
            x_dist = samples_df.iloc[n_individuals * sample_id + i]['x_pos'] - samples_df.iloc[n_individuals * sample_id + j]['x_pos']
            y_dist = samples_df.iloc[n_individuals * sample_id + i]['y_pos'] - samples_df.iloc[n_individuals * sample_id + j]['y_pos']
            tot_distance = pow(pow(x_dist, 2) + pow(y_dist, 2), 0.5)
            edges_with_weights.append((i, j, tot_distance))
            distances.append(tot_distance)


        # update the network (if necessary)
        if(network):
            G.add_weighted_edges_from(edges_with_weights)

            gcc = nx.algorithms.closeness_centrality(G, distance='weight')
            cc = []
            for k, v in gcc.items():
                cc.append(v)

        for feature in features:
            val = 0
            if(feature == 'x_pos_high'):
                val = max(samples_df.iloc[start_iloc:end_iloc]['x_pos'].values)
            elif(feature == 'x_pos_low'):
                val = min(samples_df.iloc[start_iloc:end_iloc]['x_pos'].values)
            elif(feature == 'y_pos_high'):
                val = max(samples_df.iloc[start_iloc:end_iloc]['y_pos'].values)
            elif (feature == 'y_pos_low'):
                val = min(samples_df.iloc[start_iloc:end_iloc]['y_pos'].values)
            elif(feature == 'mean_distance'):
                val = np.mean(distances)
            elif(feature == 'total_distance'):
                val = np.sum(distances)
            elif(feature == 'x_vel_sum_of_squares'):
                val = np.sum(np.square(samples_df.iloc[start_iloc:end_iloc]['x_vel'].values))
            elif(feature == 'y_vel_sum_of_squares'):
                val = np.sum(np.square(samples_df.iloc[start_iloc:end_iloc]['y_vel'].values))
            elif(feature == 'x_vel_mean'):
                val = np.mean(samples_df.iloc[start_iloc:end_iloc]['x_vel'].values)
            elif(feature == 'y_vel_mean'):
                val = np.mean(samples_df.iloc[start_iloc:end_iloc]['y_vel'].values)
            elif(feature == 'mean_speed'):
                vals = np.zeros([n_individuals, 1])
                for i in range(n_individuals):
                    x_vel = samples_df.iloc[start_iloc + i]['x_vel']
                    y_vel = samples_df.iloc[start_iloc + i]['y_vel']
                    mag = pow(pow(x_vel,2) + pow(y_vel,2),0.5)
                    vals[i] = mag
                val = np.mean(vals)
            elif (feature == 'mean_graph_clustering'):
                val = nx.average_clustering(G, weight='weight')
            elif (feature == 'mean_graph_closeness_centrality'):
                val = np.mean(cc)
            elif(feature == 'min_graph_closeness_centrality'):
                val = min(cc)
            elif (feature == 'max_graph_closeness_centrality'):
                val = max(cc)
            row.append(val)


        row.append(samples_df.iloc[start_iloc]['activity_label'])

        features_df = features_df.append(pd.Series(row, index=columns), ignore_index=True)

    return features_df

def learn_activities(samples_df_with_engineered_features, model_type):
    """
    Train a model to classify activities based on engineered features and use cross-validation for accuracy.
    :param samples_df_with_engineered_features: A dataframe generated by generate_samples and engineer_features.
    :param model_type: Model to use for classification.
    :return: A trained model.
    """

def predict_activities(samples_df_with_engineered_features, model):
    """
    Classify the activity type given an unlabeled set of sample data and a trained classification model.
    :param samples_df_with_engineered_features: Dataframe of sample data without activity labels generated by
    generate_samples and engineer_features.
    :param model: A trained classification model returned by learn_activities function.
    :return: Classifications of the input data and a score for accuracy.
    """